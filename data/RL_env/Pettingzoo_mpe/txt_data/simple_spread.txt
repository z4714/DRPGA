{
    "AgentFunctions": {
        "agent_reward": "def is_collision(self, agent1, agent2): delta_pos = agent1.state.p_pos - agent2.state.p_pos; dist = np.sqrt(np.sum(np.square(delta_pos))); dist_min = agent1.size + agent2.size; return True if dist < dist_min else False \n def reward(self, agent, world): rew = 0; rew -= sum(1.0 * (self.is_collision(a, agent) and a != agent) for a in world.agents) if agent.collide else 0; return rew \n def global_reward(self, world): rew = sum(-min(np.sqrt(np.sum(np.square(a.state.p_pos - lm.state.p_pos))) for a in world.agents) for lm in world.landmarks); return rew",
        "adversary_reward": "",
        "observation": "def observation(self, agent, world): entity_pos = [entity.state.p_pos - agent.state.p_pos for entity in world.landmarks]; comm = [other.state.c for other in world.agents if other is not agent]; other_pos = [other.state.p_pos - agent.state.p_pos for other in world.agents if other is not agent]; return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + comm)"
      
    },
    "ObservationSpaces": {
      "Agent": "[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, communication]",
      "Adversary": "[]"
    },
    "ActionSpaces": {
      "Agent": "[no_action, move_left, move_right, move_down, move_up]",
      "Adversary": "[]"
    },
    "Agents":"[agent_0, agent_1, agent_2]",
    "Description": "This environment has N agents, N landmarks (default N=3). At a high level, agents must learn to cover all the landmarks while avoiding collisions.More specifically, all agents are globally rewarded based on how far the closest agent is to each landmark (sum of the minimum distances). Locally, the agents are penalized if they collide with other agents (-1 for each collision). The relative weights of these rewards can be controlled with the local_ratio parameter."
}
  