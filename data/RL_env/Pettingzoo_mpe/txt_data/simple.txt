
{
    "AgentFunctions": {
        "agent_reward": "def reward(self, agent, world): dist2 = np.sum(np.square(agent.state.p_pos - world.landmarks[0].state.p_pos)); return -dist2",   
        "observation": "def observation(self, agent, world): entity_pos = [entity.state.p_pos - agent.state.p_pos for entity in world.landmarks]; return np.concatenate([agent.state.p_vel] + entity_pos)"
    },
    "ObservationSpaces": {
        "Agent": "[self_vel, landmark_rel_position]"
    },
    "ActionSpaces": {
        "Agent": "[no_action, move_left, move_right, move_down, move_up]"
    },
    "Agents":"[agent_0]",
    "Description": "In this environment a single agent sees a landmark position and is rewarded based on how close it gets to the landmark (Euclidean distance). This is not a multiagent environment, and is primarily intended for debugging purposes."
  }
  