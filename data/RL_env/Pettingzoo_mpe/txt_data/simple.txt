{
    "AgentFunctions": [
        {
            "FunctionName": "agent_reward",
            "RewardCalculation": "def reward(self, agent, world): dist2 = np.sum(np.square(agent.state.p_pos - world.landmarks[0].state.p_pos)); return -dist2"
        },
        {
            "FunctionName": "observation",
            "ObservationCalculation": "def observation(self, agent, world): entity_pos = [entity.state.p_pos - agent.state.p_pos for entity in world.landmarks]; return np.concatenate([agent.state.p_vel] + entity_pos)"
        }
    ],
    "ObservationSpaces": {
        "Agent": "[self_vel, landmark_rel_position]",
        
    },
    "ActionSpaces": {
        "Agent": "[no_action, move_left, move_right, move_down, move_up]",
    },

    "Descriptio": "In this environment a single agent sees a landmark position and is rewarded based on how close it gets to the landmark (Euclidean distance). This is not a multiagent environment, and is primarily intended for debugging purposes."
}