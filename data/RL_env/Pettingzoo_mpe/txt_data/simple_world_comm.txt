{
    "AgentFunctions": {
        "agent_reward": "def agent_reward(self, agent, world): rew = 0; shape = False; adversaries = self.adversaries(world); rew += sum(0.1 * np.sqrt(np.sum(np.square(agent.state.p_pos - adv.state.p_pos))) for adv in adversaries) if shape else 0; rew -= sum(5 for a in adversaries if agent.collide and self.is_collision(a, agent)); bound = lambda x: 0 if x < 0.9 else (x - 0.9) * 10 if x < 1.0 else min(np.exp(2 * x - 2), 10); rew -= sum(2 * bound(abs(agent.state.p_pos[p])) for p in range(world.dim_p)); rew += sum(2 for food in world.food if self.is_collision(agent, food)); rew -= 0.05 * min(np.sqrt(np.sum(np.square(food.state.p_pos - agent.state.p_pos))) for food in world.food); return rew  # Agents are rewarded based on minimum agent distance to each landmark",
        "adversary_reward": "def adversary_reward(self, agent, world): rew = 0; shape = True; agents = self.good_agents(world); adversaries = self.adversaries(world); rew -= 0.1 * min(np.sqrt(np.sum(np.square(a.state.p_pos - agent.state.p_pos))) for a in agents) if shape else 0; rew += sum(5 for ag in agents for adv in adversaries if agent.collide and self.is_collision(ag, adv)); return rew  # Agents are rewarded based on minimum agent distance to each landmark",
        "observation": "def observation(self, agent, world): entity_pos = [entity.state.p_pos - agent.state.p_pos for entity in world.landmarks if not entity.boundary]; in_forest = [np.array([-1]) for _ in range(len(world.forests))]; inf = [False for _ in range(len(world.forests))]; in_forest, inf = [np.array([1]) if self.is_collision(agent, world.forests[i]) else np.array([-1]) for i in range(len(world.forests))], [self.is_collision(agent, world.forests[i]) for i in range(len(world.forests))]; food_pos = [entity.state.p_pos - agent.state.p_pos for entity in world.food if not entity.boundary]; comm, other_pos, other_vel = [world.agents[0].state.c], [], []; for other in world.agents: if other is not agent: oth_f = [self.is_collision(other, world.forests[i]) for i in range(len(world.forests))]; for i in range(len(world.forests)): if inf[i] and oth_f[i]: other_pos.append(other.state.p_pos - agent.state.p_pos); other_vel.append(other.state.p_vel) if not other.adversary else None; break; else: other_pos.append(other.state.p_pos - agent.state.p_pos if (not any(inf) and not any(oth_f)) or agent.leader else [0, 0]); other_vel.append(other.state.p_vel if (not any(inf) and not any(oth_f)) or agent.leader or not other.adversary else [0, 0]) if not other.adversary else None; prey_forest = [np.array([1]) if any([self.is_collision(a, f) for f in world.forests]) else np.array([-1]) for a in self.good_agents(world)]; prey_forest_lead = [np.array([1]) if any([self.is_collision(a, f) for a in self.good_agents(world)]) else np.array([-1]) for f in world.forests]; return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + other_vel + in_forest + comm) if agent.adversary and not agent.leader else np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + other_vel + in_forest + comm) if agent.leader else np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + in_forest + other_vel) def observation2(self, agent, world): entity_pos = [entity.state.p_pos - agent.state.p_pos for entity in world.landmarks if not entity.boundary]; food_pos = [entity.state.p_pos - agent.state.p_pos for entity in world.food if not entity.boundary]; comm, other_pos, other_vel = [other.state.c for other in world.agents if other is not agent], [other.state.p_pos - agent.state.p_pos for other in world.agents if other is not agent], [other.state.p_vel for other in world.agents if other is not agent and not other.adversary]; return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + other_vel)"
      
    },
    "ObservationSpaces": {
      "Agent": "[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities, self_in_forest]",
      "Adversary": "Normal adversary observations:[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities, self_in_forest, leader_comm];Adversary leader observations: [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities, leader_comm]"
    },
    "ActionSpaces": {
      "Agent": "[no_action, move_left, move_right, move_down, move_up]",
      "Adversary": "Normal adversary action space: [no_action, move_left, move_right, move_down, move_up]; Adversary leader discrete action space: [say_0, say_1, say_2, say_3] X [no_action, move_left, move_right, move_down, move_up]"
    },
    "Agents":"[leadadversary_0, adversary_0, adversary_1, adversary_3, agent_0, agent_1]",
    "Description": "This environment is similar to simple_tag, except there is food (small blue balls) that the good agents are rewarded for being near, there are ‘forests’ that hide agents inside from being seen, and there is a ‘leader adversary’ that can see the agents at all times and can communicate with the other adversaries to help coordinate the chase. By default, there are 2 good agents, 3 adversaries, 1 obstacles, 2 foods, and 2 forests.In particular, the good agents reward, is -5 for every collision with an adversary, -2 x bound by the bound function described in simple_tag, +2 for every collision with a food, and -0.05 x minimum distance to any food. The adversarial agents are rewarded +5 for collisions and -0.1 x minimum distance to a good agent."
}
  