{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFaceH4/CodeAlpaca_20K Python split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Disk_D\\programming_software\\AI\\Anaconda3\\envs\\pettingZoo_Langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import  load_dataset\n",
    "\n",
    "h4ca20k = load_dataset(\"HuggingFaceH4/CodeAlpaca_20K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 18019\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 2003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(h4ca20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 18019\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(h4ca20k[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key=open('./api/apikey/glmkeys.txt', 'r').read().strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_py(prompts):\n",
    "    py_dataset = Dataset.from_dict({})\n",
    "    for i in prompts:\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"glm-4\", \n",
    "    \n",
    "            messages=[\n",
    "\n",
    "\n",
    "                {\"role\": \"system\", \"content\": \"你需要判断用户接下来提供的内容是否和python有关。有关请回答1,无关请回答0\"},\n",
    "                {\"role\": \"user\", \"content\": i[\"prompt\"]},\n",
    "                \n",
    "        \n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if(response.choices[0].message.content == \"1\"):\n",
    "            py_dataset.add_item(i)\n",
    "\n",
    "    return py_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts  = h4ca20k[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = Features({\n",
    "    'prompt': Value('string'),\n",
    "    'completion': Value('string')\n",
    "})\n",
    "\n",
    "\n",
    "py_dataset = Dataset.from_dict({'prompt':[], 'completion':[]},features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "101\n",
      "201\n",
      "301\n",
      "401\n",
      "501\n",
      "601\n",
      "701\n",
      "801\n",
      "901\n",
      "1001\n",
      "1101\n",
      "1201\n",
      "1301\n",
      "1401\n",
      "1501\n",
      "1601\n",
      "1701\n",
      "1801\n",
      "1901\n",
      "2001\n",
      "2101\n",
      "2201\n",
      "2301\n",
      "2401\n",
      "2501\n",
      "2601\n",
      "2701\n",
      "2801\n",
      "2901\n",
      "3001\n",
      "3101\n",
      "3201\n",
      "3301\n",
      "3401\n",
      "3501\n",
      "3601\n",
      "3701\n",
      "3801\n",
      "3901\n",
      "4001\n",
      "4101\n",
      "4201\n",
      "4301\n",
      "4401\n",
      "4501\n",
      "4601\n",
      "4701\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cot = 0\n",
    "for i in prompts:\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\", \n",
    "    \n",
    "        messages=[\n",
    "\n",
    "\n",
    "            {\"role\": \"system\", \"content\": \"你需要判断用户接下来提供的内容是否和python有关。有关请回答1,无关请回答0\"},\n",
    "            {\"role\": \"user\", \"content\": i[\"prompt\"]},\n",
    "                \n",
    "        \n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if (response.choices[0].message.content == \"1\"):\n",
    "        \n",
    "        \n",
    "        if not(cot % 100):\n",
    "            cot = cot+1\n",
    "            print(cot)\n",
    "            continue\n",
    "        cot = cot+1\n",
    "\n",
    "        py_dataset = concatenate_datasets([py_dataset,Dataset.from_dict({'prompt':[i['prompt']],'completion':[i['completion']]})])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 4777\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(py_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 548\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(py_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 548/548 [00:00<00:00, 42153.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "py_dataset.save_to_disk('./data/codeNLU/h4ca20k_py/h4ca20k_py_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_test = py_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_train = py_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4777/4777 [00:00<00:00, 135963.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "py_train.save_to_disk('./data/codeNLU/h4ca20k_py/h4ca20k_py_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "h4ca20k_pytest = load_from_disk(\"./data/codeNLU/h4ca20k_py/h4ca20k_py_test.parquet\")\n",
    "\n",
    "pytest_table = pa.Table.from_pandas(h4ca20k_pytest.to_pandas())\n",
    "\n",
    "pq.write_table(pytest_table, './data/codeNLU/h4ca20k_py/h4ca20k_py_test_.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4ca20k_pytrain = load_from_disk(\"./data/codeNLU/h4ca20k_py/h4ca20k_py_train.parquet\")\n",
    "\n",
    "pytrain_table = pa.Table.from_pandas(h4ca20k_pytrain.to_pandas())\n",
    "\n",
    "pq.write_table(pytrain_table, './data/codeNLU/h4ca20k_py/h4ca20k_py_train_.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4777/4777 [00:00<00:00, 118057.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 548/548 [00:00<00:00, 90953.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': py_train,\n",
    "    'test': py_test\n",
    "})\n",
    "\n",
    "\n",
    "dataset_dict.save_to_disk(\"./data/codeNLU/h4ca20k_py/h4ca20k_py.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 4777\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 548\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = dataset_dict['train'].to_pandas()\n",
    "test_df = dataset_dict['test'].to_pandas()\n",
    "train_df['partition'] = 'train'\n",
    "test_df['partition'] = 'test'\n",
    "\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "\n",
    "\n",
    "combined_df.to_parquet('./data/codeNLU/h4ca20k_py/h4ca20k_py_.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 84.33ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.36s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 166.22ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/graycatHCO3/CodeAlpaca-20K-Python/commit/38048a8242c7b8745701cf970aa389c00308c5f5', commit_message='Upload dataset', commit_description='', oid='38048a8242c7b8745701cf970aa389c00308c5f5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict.push_to_hub(\"graycatHCO3/CodeAlpaca-20K-Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('pettingZoo_Langchain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afdf63005a8e15303df45d0cecdfecb72eda57fa0cd442cd104881763b83185d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
