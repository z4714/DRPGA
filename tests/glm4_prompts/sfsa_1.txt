现在有一个主题与DND类似的沙河游戏，里面三个角色，其中两个角色叫做agent_0和agent_1，另一个角色叫做adversary_0。adversary_1不知道城市的位置，而agent_0与agent_1知道。三者都希望前往城市，但agent_0和agent_1需要想方设法阻止adversary_0知道城市的位置。最后得分的计算方式如下：

```
   def agent_reward(self, agent, world):
        # Rewarded based on how close any good agent is to the goal landmark, and how far the adversary is from it
        shaped_reward = True
        shaped_adv_reward = True

        # Calculate negative reward for adversary
        adversary_agents = self.adversaries(world)
        if shaped_adv_reward:  # distance-based adversary reward
            adv_rew = sum(
                np.sqrt(np.sum(np.square(a.state.p_pos - a.goal_a.state.p_pos)))
                for a in adversary_agents
            )
        else:  # proximity-based adversary reward (binary)
            adv_rew = 0
            for a in adversary_agents:
                if (
                    np.sqrt(np.sum(np.square(a.state.p_pos - a.goal_a.state.p_pos)))
                    < 2 * a.goal_a.size
                ):
                    adv_rew -= 5

        # Calculate positive reward for agents
        good_agents = self.good_agents(world)
        if shaped_reward:  # distance-based agent reward
            pos_rew = -min(
                np.sqrt(np.sum(np.square(a.state.p_pos - a.goal_a.state.p_pos)))
                for a in good_agents
            )
        else:  # proximity-based agent reward (binary)
            pos_rew = 0
            if (
                min(
                    np.sqrt(np.sum(np.square(a.state.p_pos - a.goal_a.state.p_pos)))
                    for a in good_agents
                )
                < 2 * agent.goal_a.size
            ):
                pos_rew += 5
            pos_rew -= min(
                np.sqrt(np.sum(np.square(a.state.p_pos - a.goal_a.state.p_pos)))
                for a in good_agents
            )
        return pos_rew + adv_rew

    def adversary_reward(self, agent, world):
        # Rewarded based on proximity to the goal landmark
        shaped_reward = True
        if shaped_reward:  # distance-based reward
            return -np.sqrt(
                np.sum(np.square(agent.state.p_pos - agent.goal_a.state.p_pos))
            )
        else:  # proximity-based reward (binary)
            adv_rew = 0
            if (
                np.sqrt(np.sum(np.square(agent.state.p_pos - agent.goal_a.state.p_pos)))
                < 2 * agent.goal_a.size
            ):
                adv_rew += 5
            return adv_rew
```

三者之间随时可以对话并知道其他人的位置。我将给出当前的observations, 三者的action。observation的计算方式如下：

```
    def observation(self, agent, world):
        # get positions of all entities in this agent's reference frame
        entity_pos = []
        for entity in world.landmarks:
            entity_pos.append(entity.state.p_pos - agent.state.p_pos)
        # entity colors
        entity_color = []
        for entity in world.landmarks:
            entity_color.append(entity.color)
        # communication of all other agents
        other_pos = []
        for other in world.agents:
            if other is agent:
                continue
            other_pos.append(other.state.p_pos - agent.state.p_pos)

        if not agent.adversary:
            return np.concatenate(
                [agent.goal_a.state.p_pos - agent.state.p_pos] + entity_pos + other_pos
            )
        else:
            return np.concatenate(entity_pos + other_pos)
```

observation和action的具体结构如下;

```
Agent observation space: `[self_pos, self_vel, goal_rel_position, landmark_rel_position, other_agent_rel_positions]`

Adversary observation space: `[landmark_rel_position, other_agents_rel_positions]`

Agent action space: `[no_action, move_left, move_right, move_down, move_up]`

Adversary action space: `[no_action, move_left, move_right, move_down, move_up]`
```

总行动轮数由用户给出“rounds=”。三者之间的对话方式为英文，请根据observation和action信息，生成符合情景的对话文字。一个生成的示例如下：

请按照这样的格式帮我生成局内对话，一个json的{}表示一回合：{
    "state_0": {
      "adversary_0": [0.69437176, -1.4609686, 0.023015, -0.97559977, 0.51697916, -1.5288411, 1.0734879, -0.19491644],
      "agent_0": [0.17739256, 0.06787257, 0.17739256, 0.06787257, -0.4939642, 0.5532414, -0.51697916, 1.5288411, 0.55650866, 1.3339247],
      "agent_1": [-0.3791161, -1.2660521, -0.3791161, -1.2660521, -1.0504729, -0.78068334, -1.0734879, 0.19491644, -0.55650866, -1.3339247]
    },
    "actions_0": {
      "adversary_0": 2,
      "agent_0": 4,
      "agent_1": 3
    },
    "state_1": {
      "adversary_0": [0.69437176, -1.4609686, 0.023015, -0.97559977, 0.51697916, -1.5288411, 1.0734879, -0.19491644],
      "agent_0": [0.17739256, 0.06787257, 0.17739256, 0.06787257, -0.4939642, 0.5532414, -0.51697916, 1.5288411, 0.55650866, 1.3339247],
      "agent_1": [-0.3791161, -1.2660521, -0.3791161, -1.2660521, -1.0504729, -0.78068334, -1.0734879, 0.19491644, -0.55650866, -1.3339247]
    },
    "reward_0": {
      "adversary_0": -1.6175850136777643,
      "agent_0": 1.4276513321984416,
      "agent_1": 1.4276513321984416
    },
    "adversary_0(Thief)": {
      "dialogues": [
        [],
        [],
        [],
        [
          "It seems like this area is hiding many secrets. Have you found any clues?",
          "Do you really think I'm so easily fooled? I know you have secrets, and I'll uncover the truth. But, maybe we can come to some sort of agreement—you give me some clues, and I can guarantee not to pose a threat to you once you find the treasure."
        ]
      ]
    },
    "agent_0(Elf Wizard)": {
      "dialogues": [
        [],
        [],
        [
          "I feel like every step we take is being watched. That thief, his network of informants is more extensive than we imagined. We need to be even more cautious in guarding this secret."
        ],
        [
          "Indeed, exploring to the north seems to yield unexpected discoveries.",
          "We're not ones to trust easily, especially someone like you, a thief. Our plan is to find that ancient city, not to share our discoveries with potential enemies. However, we'll consider your proposal, as long as you can prove your sincerity first."
        ]
      ]
    },
    "agent_1(Human Warrior)": {
      "dialogues": [
        [],
        [
          "I agree. We must find a way to mislead him, to lead him away from the true treasure's location. Perhaps we can intentionally leak some false information, leading him to dangerous places."
        ],
        [],
        [
          "I agree, there are certainly some noteworthy places to the north, but I think the east is also worth exploring.",
          "And we have our own resources and means to protect ourselves. However, if you can truly assist us in our expedition, perhaps we can share some less critical information."
        ]
      ]
    },
    "order": [
      [0, 1, 2, 1, 2, 0, 1, 2],
      [3, 3],
      [2, 3, 3],
      [1, 3, 3]
    ]
  }

